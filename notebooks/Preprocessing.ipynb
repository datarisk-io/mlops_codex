{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLOpsPreprocessing\n",
    "\n",
    "This notebook give a exemple on how to use MLOps to deploy a preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlops_codex.preprocessing import MLOpsPreprocessingClient\n",
    "from mlops_codex.model import MLOpsModelClient"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLOpsPreprocessingClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MLOpsPreprocessingClient()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating sync pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './samples/syncPreprocessing/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sync_preprocessing = client.create(\n",
    "    preprocessing_name='Teste preprocessing Sync', # model_name\n",
    "    preprocessing_reference='process', # name of the scoring function\n",
    "    source_file=PATH+'app.py', # Path of the source file\n",
    "    requirements_file=PATH+'requirements.txt', # Path of the requirements file, \n",
    "    schema=PATH+'schema.json', # Path of the schema file, but it could be a dict (only required for Sync models)\n",
    "    # env=PATH+'.env'  #  File for env variables (this will be encrypted in the server)\n",
    "    # extra_files=[PATH+'utils.py'], # List with extra files paths that should be uploaded along (they will be all in the same folder)\n",
    "    python_version='3.9', # Can be 3.8 to 3.10\n",
    "    operation=\"Sync\", # Can be Sync or Async\n",
    "    group='groupname' # Model group (create one using the client)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sync_preprocessing.set_token('29d9d82e09bb4c11b9cd4ce4e36e6c58')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = sync_preprocessing.run(\n",
    "    data={'variable' : 100}\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating async pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './samples/asyncPreprocessing/'\n",
    "\n",
    "async_preprocessing = client.create(\n",
    "    preprocessing_name='Teste preprocessing Async', # preprocessing_name\n",
    "    preprocessing_reference='build_df', # name of the scoring function\n",
    "    source_file=PATH+'app.py', # Path of the source file\n",
    "    requirements_file=PATH+'requirements.txt', # Path of the requirements file, \n",
    "    # env=PATH+'.env',  #  File for env variables (this will be encrypted in the server)\n",
    "    # extra_files=[PATH+'input.csv'], # List with extra files paths that should be uploaded along (they will be all in the same folder)\n",
    "    schema=PATH+'schema.csv',\n",
    "    python_version='3.9', # Can be 3.8 to 3.10\n",
    "    operation=\"Async\", # Can be Sync or Async\n",
    "    group='groupname', # Model group (create one using the client)\n",
    "    input_type='csv',\n",
    "    wait_for_ready=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async_preprocessing.set_token('29d9d82e09bb4c11b9cd4ce4e36e6c58')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = async_preprocessing.run(data=PATH+'input.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.get_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.wait_ready()\n",
    "execution.download_result()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access created pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.search_preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing = client.get_preprocessing(preprocessing_id='Sa79236b3dfc4f22a502e816a07dab382cee6327a5334c5bbba13c456233b8c4', group='groupname')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access created executions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_execution = async_preprocessing.get_preprocessing_execution(exec_id='2')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "execution_4.download_result()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using preprocessing with models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_client = MLOpsModelClient()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sync Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sync_model = model_client.get_model(group='groupname', model_id='M7abe6af98484948ad63f3ad03f25b6496a93f06e23c4ffbaa43eba0f6a1bb91')\n",
    "\n",
    "sync_model.set_token('29d9d82e09bb4c11b9cd4ce4e36e6c58')\n",
    "\n",
    "data = {\n",
    " \"mean_radius\": 17.99,\n",
    " \"mean_texture\": 10.38,\n",
    " \"mean_perimeter\": 122.8,\n",
    " \"mean_area\": 1001.0,\n",
    " \"mean_smoothness\": 0.1184,\n",
    " \"mean_compactness\": 0.2776,\n",
    " \"mean_concavity\": 0.3001,\n",
    " \"mean_concave_points\": 0.1471,\n",
    " \"mean_symmetry\": 0.2419,\n",
    " \"mean_fractal_dimension\": 0.07871,\n",
    " \"radius_error\": 1.095,\n",
    " \"texture_error\": 0.9053,\n",
    " \"perimeter_error\": 8.589,\n",
    " \"area_error\": 153.4,\n",
    " \"smoothness_error\": 0.006399,\n",
    " \"compactness_error\": 0.04904,\n",
    " \"concavity_error\": 0.05373,\n",
    " \"concave_points_error\": 0.01587,\n",
    " \"symmetry_error\": 0.03003,\n",
    " \"fractal_dimension_error\": 0.006193,\n",
    " \"worst_radius\": 25.38,\n",
    " \"worst_texture\": 17.33,\n",
    " \"worst_perimeter\": 184.6,\n",
    " \"worst_area\": 2019.0,\n",
    " \"worst_smoothness\": 0.1622,\n",
    " \"worst_compactness\": 0.6656,\n",
    " \"worst_concavity\": 0.7119,\n",
    " \"worst_concave_points\": 0.2654,\n",
    " \"worst_symmetry\": 0.4601,\n",
    " \"worst_fractal_dimension\": 0.1189\n",
    "}\n",
    "\n",
    "sync_model.predict(data=data, preprocessing=sync_preprocessing)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Async Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async_model = model_client.get_model(group='groupname', model_id='Me6ebaa539cb4a738a66fc52fc34b5422a8c6ae3942b4ca1868624cfda964db3')\n",
    "\n",
    "PATH = './samples/asyncModel/'\n",
    "\n",
    "async_model.set_token('29d9d82e09bb4c11b9cd4ce4e36e6c58')\n",
    "\n",
    "execution = async_model.predict(data=PATH+'input.csv', preprocessing=async_preprocessing)\n",
    "execution.wait_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.download_result()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "-----"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## New preprocessing\n",
    "\n",
    "We're rebuilding the process module. The main feature is the end multiples datasets to MLOps server. Check the code below"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T20:00:10.067895Z",
     "start_time": "2025-01-27T19:57:35.553380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from mlops_codex.preprocessing import MLOpsPreprocessingAsyncV2Client\n",
    "\n",
    "client = MLOpsPreprocessingAsyncV2Client()\n",
    "\n",
    "PATH = \"./samples/asyncPreprocessingMultiple/\"\n",
    "\n",
    "schemas = [\n",
    "    (\"base_cadastral\", PATH+'base_cadastral.csv'),\n",
    "    (\"base_pagamentos\", PATH+'base_pagamentos.csv'),\n",
    "    (\"base_info\", PATH+'base_info.csv'),\n",
    "]\n",
    "\n",
    "preprocess = client.create(\n",
    "    name=\"test_preprocessing\",\n",
    "    group=\"datarisk\",\n",
    "    schema_files_path=schemas,\n",
    "    script_path=PATH+'app.py',\n",
    "    entrypoint_function_name=\"build_df\",\n",
    "    python_version='3.9',\n",
    "    requirements_path=PATH+'requirements.txt',\n",
    "    wait_read=True\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "January 27, 2025 | INFO: __init__ Loading .env\n",
      "January 27, 2025 | INFO: __init__ Successfully connected to MLOps\n",
      "January 27, 2025 | INFO: create Creating preprocessing for preprocessing hash\n",
      " Preprocessing hash = Sabf4c60b7a54759bf205e3eb3325e55deabf943a6a54b8cbf23557e60bfd937\n",
      "January 27, 2025 | INFO: create Created dataset hash D7c87c24d5a141baab6ae75f2d47a7737628a7a2c2814aa1b03ba5a0f94a2db5 with name base_cadastral\n",
      "January 27, 2025 | INFO: create Created dataset hash D20bfd9cbaf840df83da7abddeb1a9429f700a6733d54e36873ab9b8c7b6b981 with name base_pagamentos\n",
      "January 27, 2025 | INFO: create Created dataset hash D6c4af5f365e45caba7fe8343097f566e4a782bd0ac2455f910fbb90df272ae4 with name base_info\n",
      "January 27, 2025 | INFO: create Schema files uploaded\n",
      "January 27, 2025 | INFO: create Script file uploaded\n",
      "January 27, 2025 | INFO: create Requirements file uploaded\n",
      "January 27, 2025 | INFO: create Hosting preprocessing script\n",
      "Waiting for preprocessing script to finish.....January 27, 2025 | INFO: wait \n",
      "Preprocessing script finished successfully\n",
      "January 27, 2025 | INFO: create Successfully hosted preprocessing script\n",
      "January 27, 2025 | INFO: __init__ Loading .env\n",
      "January 27, 2025 | INFO: __init__ Successfully connected to MLOps\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T20:01:47.653615Z",
     "start_time": "2025-01-27T20:00:18.728989Z"
    }
   },
   "cell_type": "code",
   "source": [
    "inputs = [\n",
    "    (\"base_cadastral\", PATH+'base_cadastral.csv'),\n",
    "    (\"base_pagamentos\", PATH+'base_pagamentos.csv'),\n",
    "    (\"base_info\", PATH+'base_info.csv'),\n",
    "]\n",
    "\n",
    "preprocess.run(\n",
    "    input_files=inputs,\n",
    "    wait_read=True\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "January 27, 2025 | INFO: register_execution Registered execution for preprocessing hash Sabf4c60b7a54759bf205e3eb3325e55deabf943a6a54b8cbf23557e60bfd937\n",
      " Message = Preprocess Execution '16' created\n",
      "January 27, 2025 | INFO: run Preprocessing script execution Sabf4c60b7a54759bf205e3eb3325e55deabf943a6a54b8cbf23557e60bfd937 is registered. Execution ID = 16\n",
      "January 27, 2025 | INFO: run Uploaded input file ('base_cadastral', './samples/asyncPreprocessingMultiple/base_cadastral.csv') - Output Hash D17861f1574f419a9350185dc01dd1658cd932d271234f8e98407ead4834311a\n",
      "January 27, 2025 | INFO: run Uploaded input file ('base_pagamentos', './samples/asyncPreprocessingMultiple/base_pagamentos.csv') - Output Hash De1f49c6b5b84d1d9a42bc144d2948a5b87cd849bb8449d792b16c3a90112889\n",
      "January 27, 2025 | INFO: run Uploaded input file ('base_info', './samples/asyncPreprocessingMultiple/base_info.csv') - Output Hash D7e77efaa50e4eda9293be071dada3115038a38638f04f5b93e3e3d5c64ecbf1\n",
      "January 27, 2025 | INFO: run Started preprocessing script execution 16 - Hash Sabf4c60b7a54759bf205e3eb3325e55deabf943a6a54b8cbf23557e60bfd937\n",
      "Waiting for preprocessing script to finish..January 27, 2025 | INFO: __wait_for_execution Preprocessing script finished successfully\n",
      "January 27, 2025 | INFO: run Script finished successfully. Consider downloading the results using the 'download()' method.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "execution_id = 1\n",
    "\n",
    "preprocess.download(execution_id=execution_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops-neomaril-codex-c4z0dHNl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
