{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLOps Models\n",
    "\n",
    "This notebook give a exemple on how to use MLOps to deploy a ML model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLOpsModelClient\n",
    "\n",
    "It's where you can manage your models"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Start the client. We are reading the credentials in the NEOMARIL_TOKEN env variable\n",
    "from mlops_codex.model import MLOpsModelClient\n",
    "client = MLOpsModelClient(\n",
    "    login=\"<email>\",\n",
    "    password=\"<tenant>\",\n",
    "    tenant=\"<tenant>\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLOpsModel\n",
    "\n",
    "It's where you can use your model after you fetch it with the client (or created a new one)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Or create a new one\n",
    "\n",
    "PATH = './samples/syncModel/'\n",
    "\n",
    "# Deploying a new model\n",
    "model = client.create_model(\n",
    "    model_name='Teste notebook Sync', # model_name\n",
    "    model_reference='score', # name of the scoring function\n",
    "    source_file=PATH+'app.py', # Path of the source file\n",
    "    model_file=PATH+'model.pkl', # Path of the model pkl file, \n",
    "    requirements_file=PATH+'requirements.txt', # Path of the requirements file, \n",
    "    schema=PATH+'schema.json', # Path of the schema file, but it could be a dict (only required for Sync models)\n",
    "    # env=PATH+'.env'  #  File for env variables (this will be encrypted in the server)\n",
    "    # extra_files=[PATH+'utils.py'], # List with extra files paths that should be uploaded along (they will be all in the same folder)\n",
    "    python_version='3.9', # Can be 3.8 to 3.10\n",
    "    operation=\"Sync\", # Can be Sync or Async\n",
    "    group='<group>' # Model group (create one using the client)\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# set group token for this model (you can also add this token in each .predict call or as a env variable NEOMARIL_GROUP_TOKEN)\n",
    "model.set_token('<group_token>')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Get all information about your model\n",
    "model.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Disable your model\n",
    "model.disable()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# However, remember to restart your model if you wish to use it again\n",
    "model.restart_model(wait_for_ready=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Run predictions with the predict method, or just call it with the model object\n",
    "data = {\n",
    " \"mean_radius\": 17.99,\n",
    " \"mean_texture\": 10.38,\n",
    " \"mean_perimeter\": 122.8,\n",
    " \"mean_area\": 1001.0,\n",
    " \"mean_smoothness\": 0.1184,\n",
    " \"mean_compactness\": 0.2776,\n",
    " \"mean_concavity\": 0.3001,\n",
    " \"mean_concave_points\": 0.1471,\n",
    " \"mean_symmetry\": 0.2419,\n",
    " \"mean_fractal_dimension\": 0.07871,\n",
    " \"radius_error\": 1.095,\n",
    " \"texture_error\": 0.9053,\n",
    " \"perimeter_error\": 8.589,\n",
    " \"area_error\": 153.4,\n",
    " \"smoothness_error\": 0.006399,\n",
    " \"compactness_error\": 0.04904,\n",
    " \"concavity_error\": 0.05373,\n",
    " \"concave_points_error\": 0.01587,\n",
    " \"symmetry_error\": 0.03003,\n",
    " \"fractal_dimension_error\": 0.006193,\n",
    " \"worst_radius\": 25.38,\n",
    " \"worst_texture\": 17.33,\n",
    " \"worst_perimeter\": 184.6,\n",
    " \"worst_area\": 2019.0,\n",
    " \"worst_smoothness\": 0.1622,\n",
    " \"worst_compactness\": 0.6656,\n",
    " \"worst_concavity\": 0.7119,\n",
    " \"worst_concave_points\": 0.2654,\n",
    " \"worst_symmetry\": 0.4601,\n",
    " \"worst_fractal_dimension\": 0.1189\n",
    "}\n",
    "\n",
    "model.predict(json_data=data)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# # We can also add a monitoring configuration for the model\n",
    "\n",
    "PATH = './samples/monitoring/'\n",
    "\n",
    "model.register_monitoring(preprocess_reference='parse', shap_reference='get_shap',\n",
    "                          configuration_file=PATH + 'conf.json', preprocess_file=PATH + 'preprocess_sync.py',\n",
    "                          requirements_file=PATH + 'requirements.txt')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLOpsAsyncModelExecution\n",
    "We can create async models as well to send bigger data to predict"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "PATH = './samples/asyncModel/'\n",
    "\n",
    "# Deploying a new model\n",
    "model = client.create_model(\n",
    "    model_name='Teste notebook Async', # model_name\n",
    "    model_reference='score', # name of the scoring function\n",
    "    source_file=PATH+'app.py', # Path of the source file\n",
    "    model_file=PATH+'model.pkl', # Path of the model pkl file, \n",
    "    requirements_file=PATH+'requirements.txt', # Path of the requirements file, \n",
    "    schema=PATH+'schema.csv', # Path of the schema file, but it could be a dict (only required for Sync models)\n",
    "    # env=PATH+'.env'  #  File for env variables (this will be encrypted in the server)\n",
    "    # extra_files=[PATH+'utils.py'], # List with extra files paths that should be uploaded along (they will be all in the same folder)\n",
    "    python_version='3.9', # Can be 3.8 to 3.10\n",
    "    operation=\"Async\", # Can be Sync or Async\n",
    "    input_type='csv',# Can be json or csv or parquet\n",
    "    group='<group>' # Model group (create one using the client)\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "PATH = './samples/asyncModel/'\n",
    "execution = model.predict(data=PATH+'input.csv', group_token='<group_token>')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "execution.execution_data"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "execution.get_status()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "execution.wait_ready()\n",
    "execution.download_result()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops-neomaril-codex-c4z0dHNl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
